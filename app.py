import json
import numpy as np
import faiss
import streamlit as st
from sentence_transformers import SentenceTransformer
import unicodedata
import re

# ---------------------------
# T·∫¢I M√î H√åNH VIETNAMESE-EMBEDDING
# ---------------------------
model = SentenceTransformer("dangvantuan/vietnamese-embedding", device="cpu")

# ---------------------------
# H√ÄM LO·∫†I B·ªé D·∫§U TI·∫æNG VI·ªÜT
# ---------------------------
def remove_accents(text):
    text = unicodedata.normalize('NFD', text)
    text = re.sub(r'\p{Mn}', '', text)
    return text

# ---------------------------
# H√ÄM T·∫†O EMBEDDING BATCH
# ---------------------------
def get_embedding_batch(texts):
    texts = [remove_accents(t.lower()) for t in texts]  # Lo·∫°i b·ªè d·∫•u tr∆∞·ªõc khi embedding
    embeddings = model.encode(texts, normalize_embeddings=True, batch_size=16)
    return np.array(embeddings, dtype="float32")

# ---------------------------
# ƒê·ªåC D·ªÆ LI·ªÜU Q&A T·ª™ FILE chatbot.json
# ---------------------------
data_file = "chatbot.json"
with open(data_file, "r", encoding="utf-8") as f:
    qa_data = json.load(f)

# ---------------------------
# ƒê·ªåC EMBEDDING T·ª™ FILE HO·∫∂C T·∫†O M·ªöI
# ---------------------------
embedding_file = "embedding_data_vn.json"

try:
    with open(embedding_file, "r", encoding="utf-8") as f:
        embedding_data = json.load(f)
    print("‚úÖ Loaded precomputed embeddings.")
except FileNotFoundError:
    print("üöÄ Embedding file not found. Creating new embeddings...")
    questions_list = [q for qs in qa_data.values() for q in qs]
    embeddings_array = get_embedding_batch(questions_list)
    embedding_data = {q: embeddings_array[i].tolist() for i, q in enumerate(questions_list)}
    
    with open(embedding_file, "w", encoding="utf-8") as f:
        json.dump(embedding_data, f, ensure_ascii=False, indent=4)

# ---------------------------
# CHUY·ªÇN EMBEDDING T·ª™ JSON SANG NUMPY ARRAY
# ---------------------------
questions_list = list(embedding_data.keys())
embeddings_list = [embedding_data[question] for question in questions_list]
embeddings_array = np.array(embeddings_list, dtype="float32")

# ---------------------------
# CHU·∫®N H√ìA VECTORS (L2 normalization)
# ---------------------------
faiss.normalize_L2(embeddings_array)

# ---------------------------
# X√ÇY D·ª∞NG FAISS INDEX
# ---------------------------
d = embeddings_array.shape[1]
index = faiss.IndexFlatIP(d)
index.add(embeddings_array)

index_to_question = {i: questions_list[i] for i in range(len(questions_list))}

# ---------------------------
# H√ÄM X·ª¨ L√ù TRUY V·∫§N Q&A S·ª¨ D·ª§NG FAISS
# ---------------------------
def answer_query_faiss(user_query, similarity_threshold=0.2):
    query_emb = get_embedding_batch([user_query])
    k = 1
    distances, indices = index.search(query_emb, k)
    
    best_score = distances[0][0]
    best_index = indices[0][0]

    if best_score < similarity_threshold:
        return "Ch√∫ng t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n.", None, None
    
    best_question = index_to_question[best_index]
    
    for key, value in qa_data.items():
        if best_question in value:
            return key, best_question, best_score
    
    return "C√¢u h·ªèi kh√¥ng kh·ªõp v·ªõi d·ªØ li·ªáu, vui l√≤ng th·ª≠ l·∫°i!", None, None

# ---------------------------
# STREAMLIT UI
# ---------------------------
st.title("Chatbot tra c·ª©u th√¥ng tin c√°c k·ª≥ thi c·ªßa Empire")

user_query = st.text_input("B·∫°n h·ªèi:")

if user_query:
    answer, matched_question, similarity = answer_query_faiss(user_query)

    if matched_question:
        if isinstance(answer, str) and answer.lower().endswith((".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp")):
            st.image(answer, caption="K·∫øt qu·∫£ t√¨m th·∫•y", use_container_width=True)
        else:
            st.markdown(f"**Tr·∫£ l·ªùi:** \n\n{answer}")
    else:
        st.markdown(f"**Tr·∫£ l·ªùi:** \n\n{answer}")
